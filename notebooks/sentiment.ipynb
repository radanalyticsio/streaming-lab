{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing and sentiment analysis\n",
    "\n",
    "In this notebook, we'll show you how to analyze the sentiments (positive, negative, or neutral) of our synthetic social media updates.  There are a lot of tricky problems in preparing natural language inputs for machine learning and in training machine learning models for natural language, but we're going to ignore those because we can use some libraries that have done most of the hard work for us:\n",
    "\n",
    "1.  [https://spacy.io](spaCy) is a library that has methods and pretrained models for parsing natural language and determining parts of speech, etc., in many languages, and\n",
    "2.  [VADER](https://github.com/cjhutto/vaderSentiment) is a library that can characterize the sentiments of individual sentences.\n",
    "\n",
    "While sentiment analysis is an interesting application, we hope you'll be inspired to try other language-processing tasks with spaCy, too.  You'll also learn how to glue sophisticated language-processing code into a Spark pipeline so that you can use it to process streaming data.  \n",
    "\n",
    "The bigger lesson of this notebook is that you may not always need to train a model to add intelligence to an application:  often pretrained models or even off-the-shelf intelligent APIs are available for interesting tasks (this is particularly true for tasks like language processing and image recognition that are broadly applicable).\n",
    "\n",
    "## Setup\n",
    "\n",
    "We'll start by importing the spaCy library and telling it to load a pretrained model for English text.  The spaCy project ships [several pretrained models](https://spacy.io/models/), but we are going to use [a relatively compact model of English](https://spacy.io/models/en#en_core_web_sm) trained on web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to import the analyzer class from VADER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing text and identifying sentiments\n",
    "\n",
    "We'll look at the sentiment of some example text from Jane Austen (we picked a notably recognizable excerpt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"\"\" It is a truth universally acknowledged, that a single man in possession\n",
    "of a good fortune, must be in want of a wife.\n",
    "\n",
    "However little known the feelings or views of such a man may be on his\n",
    "first entering a neighbourhood, this truth is so well fixed in the minds\n",
    "of the surrounding families, that he is considered the rightful property\n",
    "of some one or other of their daughters. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to tell spaCy to use `english` -- the model we loaded -- to parse the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = english(sampletext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may not know what we can do with a `spacy.tokens.doc.Doc`, but since most good Python code includes documentation, we can find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Doc object:\n",
      "\n",
      "class Doc(builtins.object)\n",
      " |  A sequence of Token objects. Access sentences and named entities, export\n",
      " |  annotations to numpy arrays, losslessly serialize to compressed binary\n",
      " |  strings. The `Doc` object holds an array of `TokenC` structs. The\n",
      " |  Python-level `Token` and `Span` objects are views of this array, i.e.\n",
      " |  they don't own the data themselves.\n",
      " |  \n",
      " |  EXAMPLE: Construction 1\n",
      " |      >>> doc = nlp(u'Some text')\n",
      " |  \n",
      " |      Construction 2\n",
      " |      >>> from spacy.tokens import Doc\n",
      " |      >>> doc = Doc(nlp.vocab, words=[u'hello', u'world', u'!'],\n",
      " |                    spaces=[True, False, False])\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bytes__(...)\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      Get a `Token` or `Span` object.\n",
      " |      \n",
      " |      i (int or tuple) The index of the token, or the slice of the document\n",
      " |          to get.\n",
      " |      RETURNS (Token or Span): The token at `doc[i]]`, or the span at\n",
      " |          `doc[start : end]`.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> doc[i]\n",
      " |          Get the `Token` object at position `i`, where `i` is an integer.\n",
      " |          Negative indexing is supported, and follows the usual Python\n",
      " |          semantics, i.e. `doc[-2]` is `doc[len(doc) - 2]`.\n",
      " |      \n",
      " |          >>> doc[start : end]]\n",
      " |          Get a `Span` object, starting at position `start` and ending at\n",
      " |          position `end`, where `start` and `end` are token indices. For\n",
      " |          instance, `doc[2:5]` produces a span consisting of tokens 2, 3 and\n",
      " |          4. Stepped slices (e.g. `doc[start : end : step]`) are not\n",
      " |          supported, as `Span` objects must be contiguous (cannot have gaps).\n",
      " |          You can use negative indices and open-ended ranges, which have\n",
      " |          their normal Python semantics.\n",
      " |  \n",
      " |  __init__(...)\n",
      " |      Create a Doc object.\n",
      " |      \n",
      " |      vocab (Vocab): A vocabulary object, which must match any models you\n",
      " |          want to use (e.g. tokenizer, parser, entity recognizer).\n",
      " |      words (list or None): A list of unicode strings to add to the document\n",
      " |          as words. If `None`, defaults to empty list.\n",
      " |      spaces (list or None): A list of boolean values, of the same length as\n",
      " |          words. True means that the word is followed by a space, False means\n",
      " |          it is not. If `None`, defaults to `[True]*len(words)`\n",
      " |      user_data (dict or None): Optional extra data to attach to the Doc.\n",
      " |      RETURNS (Doc): The newly constructed object.\n",
      " |  \n",
      " |  __iter__(...)\n",
      " |      Iterate over `Token`  objects, from which the annotations can be\n",
      " |      easily accessed. This is the main way of accessing `Token` objects,\n",
      " |      which are the main way annotations are accessed from Python. If faster-\n",
      " |      than-Python speeds are required, you can instead access the annotations\n",
      " |      as a numpy array, or access the underlying C data directly from Cython.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> for token in doc\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      The number of tokens in the document.\n",
      " |      \n",
      " |      RETURNS (int): The number of tokens in the document.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> len(doc)\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __reduce__ = __reduce_cython__(...)\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__ = __setstate_cython__(...)\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __unicode__(...)\n",
      " |  \n",
      " |  char_span(...)\n",
      " |      Create a `Span` object from the slice `doc.text[start : end]`.\n",
      " |      \n",
      " |      doc (Doc): The parent document.\n",
      " |      start (int): The index of the first character of the span.\n",
      " |      end (int): The index of the first character after the span.\n",
      " |      label (uint64 or string): A label to attach to the Span, e.g. for\n",
      " |          named entities.\n",
      " |      vector (ndarray[ndim=1, dtype='float32']): A meaning representation of\n",
      " |          the span.\n",
      " |      RETURNS (Span): The newly constructed object.\n",
      " |  \n",
      " |  count_by(...)\n",
      " |      Count the frequencies of a given attribute. Produces a dict of\n",
      " |      `{attribute (int): count (ints)}` frequencies, keyed by the values of\n",
      " |      the given attribute ID.\n",
      " |      \n",
      " |      attr_id (int): The attribute ID to key the counts.\n",
      " |      RETURNS (dict): A dictionary mapping attributes to integer counts.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> from spacy import attrs\n",
      " |          >>> doc = nlp(u'apple apple orange banana')\n",
      " |          >>> tokens.count_by(attrs.ORTH)\n",
      " |          {12800L: 1, 11880L: 2, 7561L: 1}\n",
      " |          >>> tokens.to_array([attrs.ORTH])\n",
      " |          array([[11880], [11880], [7561], [12800]])\n",
      " |  \n",
      " |  extend_tensor(...)\n",
      " |      Concatenate a new tensor onto the doc.tensor object.\n",
      " |      \n",
      " |      The doc.tensor attribute holds dense feature vectors\n",
      " |      computed by the models in the pipeline. Let's say a\n",
      " |      document with 30 words has a tensor with 128 dimensions\n",
      " |      per word. doc.tensor.shape will be (30, 128). After\n",
      " |      calling doc.extend_tensor with an array of hape (30, 64),\n",
      " |      doc.tensor == (30, 192).\n",
      " |  \n",
      " |  from_array(...)\n",
      " |  \n",
      " |  from_bytes(...)\n",
      " |      Deserialize, i.e. import the document contents from a binary string.\n",
      " |      \n",
      " |      data (bytes): The string to load from.\n",
      " |      RETURNS (Doc): Itself.\n",
      " |  \n",
      " |  from_disk(...)\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it.\n",
      " |      \n",
      " |      path (unicode or Path): A path to a directory. Paths may be either\n",
      " |          strings or `Path`-like objects.\n",
      " |      RETURNS (Doc): The modified `Doc` object.\n",
      " |  \n",
      " |  get_extension(...) from builtins.type\n",
      " |  \n",
      " |  get_lca_matrix(...)\n",
      " |      Calculates the lowest common ancestor matrix for a given `Doc`.\n",
      " |      Returns LCA matrix containing the integer index of the ancestor, or -1\n",
      " |      if no common ancestor is found (ex if span excludes a necessary\n",
      " |      ancestor). Apologies about the recursion, but the impact on\n",
      " |      performance is negligible given the natural limitations on the depth\n",
      " |      of a typical human sentence.\n",
      " |  \n",
      " |  has_extension(...) from builtins.type\n",
      " |  \n",
      " |  merge(...)\n",
      " |      Retokenize the document, such that the span at\n",
      " |      `doc.text[start_idx : end_idx]` is merged into a single token. If\n",
      " |      `start_idx` and `end_idx `do not mark start and end token boundaries,\n",
      " |      the document remains unchanged.\n",
      " |      \n",
      " |      start_idx (int): Character index of the start of the slice to merge.\n",
      " |      end_idx (int): Character index after the end of the slice to merge.\n",
      " |      **attributes: Attributes to assign to the merged token. By default,\n",
      " |          attributes are inherited from the syntactic root of the span.\n",
      " |      RETURNS (Token): The newly merged token, or `None` if the start and end\n",
      " |          indices did not fall at token boundaries.\n",
      " |  \n",
      " |  print_tree(...)\n",
      " |      Returns the parse trees in JSON (dict) format.\n",
      " |      \n",
      " |      light (bool): Don't include lemmas or entities.\n",
      " |      flat (bool): Don't include arcs or modifiers.\n",
      " |      RETURNS (dict): Parse tree as dict.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> doc = nlp('Bob brought Alice the pizza. Alice ate the pizza.')\n",
      " |          >>> trees = doc.print_tree()\n",
      " |          >>> trees[1]\n",
      " |          {'modifiers': [\n",
      " |              {'modifiers': [], 'NE': 'PERSON', 'word': 'Alice',\n",
      " |              'arc': 'nsubj', 'POS_coarse': 'PROPN', 'POS_fine': 'NNP',\n",
      " |              'lemma': 'Alice'},\n",
      " |              {'modifiers': [\n",
      " |                  {'modifiers': [], 'NE': '', 'word': 'the', 'arc': 'det',\n",
      " |                  'POS_coarse': 'DET', 'POS_fine': 'DT', 'lemma': 'the'}],\n",
      " |              'NE': '', 'word': 'pizza', 'arc': 'dobj', 'POS_coarse': 'NOUN',\n",
      " |              'POS_fine': 'NN', 'lemma': 'pizza'},\n",
      " |              {'modifiers': [], 'NE': '', 'word': '.', 'arc': 'punct',\n",
      " |              'POS_coarse': 'PUNCT', 'POS_fine': '.', 'lemma': '.'}],\n",
      " |              'NE': '', 'word': 'ate', 'arc': 'ROOT', 'POS_coarse': 'VERB',\n",
      " |              'POS_fine': 'VBD', 'lemma': 'eat'}\n",
      " |  \n",
      " |  retokenize(...)\n",
      " |      Context manager to handle retokenization of the Doc.\n",
      " |      Modifications to the Doc's tokenization are stored, and then\n",
      " |      made all at once when the context manager exits. This is\n",
      " |      much more efficient, and less error-prone.\n",
      " |      \n",
      " |      All views of the Doc (Span and Token) created before the\n",
      " |      retokenization are invalidated, although they may accidentally\n",
      " |      continue to work.\n",
      " |  \n",
      " |  set_extension(...) from builtins.type\n",
      " |  \n",
      " |  similarity(...)\n",
      " |      Make a semantic similarity estimate. The default estimate is cosine\n",
      " |      similarity using an average of word vectors.\n",
      " |      \n",
      " |      other (object): The object to compare with. By default, accepts `Doc`,\n",
      " |          `Span`, `Token` and `Lexeme` objects.\n",
      " |      RETURNS (float): A scalar similarity score. Higher is more similar.\n",
      " |  \n",
      " |  to_array(...)\n",
      " |      Export given token attributes to a numpy `ndarray`.\n",
      " |      If `attr_ids` is a sequence of M attributes, the output array will be\n",
      " |      of shape `(N, M)`, where N is the length of the `Doc` (in tokens). If\n",
      " |      `attr_ids` is a single attribute, the output shape will be (N,). You\n",
      " |      can specify attributes by integer ID (e.g. spacy.attrs.LEMMA) or\n",
      " |      string name (e.g. 'LEMMA' or 'lemma').\n",
      " |      \n",
      " |      attr_ids (list[]): A list of attributes (int IDs or string names).\n",
      " |      RETURNS (numpy.ndarray[long, ndim=2]): A feature matrix, with one row\n",
      " |          per word, and one column per attribute indicated in the input\n",
      " |          `attr_ids`.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
      " |          >>> doc = nlp(text)\n",
      " |          >>> # All strings mapped to integers, for easy export to numpy\n",
      " |          >>> np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n",
      " |  \n",
      " |  to_bytes(...)\n",
      " |      Serialize, i.e. export the document contents to a binary string.\n",
      " |      \n",
      " |      RETURNS (bytes): A losslessly serialized copy of the `Doc`, including\n",
      " |          all annotations.\n",
      " |  \n",
      " |  to_disk(...)\n",
      " |      Save the current state to a directory.\n",
      " |      \n",
      " |      path (unicode or Path): A path to a directory, which will be created if\n",
      " |          it doesn't exist. Paths may be either strings or Path-like objects.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  cats\n",
      " |  \n",
      " |  doc\n",
      " |  \n",
      " |  ents\n",
      " |      Iterate over the entities in the document. Yields named-entity\n",
      " |      `Span` objects, if the entity recognizer has been applied to the\n",
      " |      document.\n",
      " |      \n",
      " |      YIELDS (Span): Entities in the document.\n",
      " |      \n",
      " |      EXAMPLE: Iterate over the span to get individual Token objects,\n",
      " |          or access the label:\n",
      " |      \n",
      " |          >>> tokens = nlp(u'Mr. Best flew to New York on Saturday morning.')\n",
      " |          >>> ents = list(tokens.ents)\n",
      " |          >>> assert ents[0].label == 346\n",
      " |          >>> assert ents[0].label_ == 'PERSON'\n",
      " |          >>> assert ents[0].orth_ == 'Best'\n",
      " |          >>> assert ents[0].text == 'Mr. Best'\n",
      " |  \n",
      " |  has_vector\n",
      " |      A boolean value indicating whether a word vector is associated with\n",
      " |      the object.\n",
      " |      \n",
      " |      RETURNS (bool): Whether a word vector is associated with the object.\n",
      " |  \n",
      " |  is_parsed\n",
      " |  \n",
      " |  is_sentenced\n",
      " |  \n",
      " |  is_tagged\n",
      " |  \n",
      " |  mem\n",
      " |  \n",
      " |  noun_chunks\n",
      " |      Iterate over the base noun phrases in the document. Yields base\n",
      " |      noun-phrase #[code Span] objects, if the document has been\n",
      " |      syntactically parsed. A base noun phrase, or \"NP chunk\", is a noun\n",
      " |      phrase that does not permit other NPs to be nested within it â€“ so no\n",
      " |      NP-level coordination, no prepositional phrases, and no relative\n",
      " |      clauses.\n",
      " |      \n",
      " |      YIELDS (Span): Noun chunks in the document.\n",
      " |  \n",
      " |  noun_chunks_iterator\n",
      " |  \n",
      " |  sentiment\n",
      " |  \n",
      " |  sents\n",
      " |      Iterate over the sentences in the document. Yields sentence `Span`\n",
      " |      objects. Sentence spans have no label. To improve accuracy on informal\n",
      " |      texts, spaCy calculates sentence boundaries from the syntactic\n",
      " |      dependency parse. If the parser is disabled, the `sents` iterator will\n",
      " |      be unavailable.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> doc = nlp(\"This is a sentence. Here's another...\")\n",
      " |          >>> assert [s.root.text for s in doc.sents] == [\"is\", \"'s\"]\n",
      " |  \n",
      " |  tensor\n",
      " |  \n",
      " |  text\n",
      " |      A unicode representation of the document text.\n",
      " |      \n",
      " |      RETURNS (unicode): The original verbatim text of the document.\n",
      " |  \n",
      " |  text_with_ws\n",
      " |      An alias of `Doc.text`, provided for duck-type compatibility with\n",
      " |      `Span` and `Token`.\n",
      " |      \n",
      " |      RETURNS (unicode): The original verbatim text of the document.\n",
      " |  \n",
      " |  user_data\n",
      " |  \n",
      " |  user_hooks\n",
      " |  \n",
      " |  user_span_hooks\n",
      " |  \n",
      " |  user_token_hooks\n",
      " |  \n",
      " |  vector\n",
      " |      A real-valued meaning representation. Defaults to an average of the\n",
      " |      token vectors.\n",
      " |      \n",
      " |      RETURNS (numpy.ndarray[ndim=1, dtype='float32']): A 1D numpy array\n",
      " |          representing the document's semantics.\n",
      " |  \n",
      " |  vector_norm\n",
      " |      The L2 norm of the document's vector representation.\n",
      " |      \n",
      " |      RETURNS (float): The L2 norm of the vector representation.\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the cool things we can do with spaCy is identify parts of speech in natural language text (in fact, we used this feature to identify words that should become hashtags in our [synthetic update generator](/notebooks/generate.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SPACE\n",
      "It PRON\n",
      "is VERB\n",
      "a DET\n",
      "truth NOUN\n",
      "universally ADV\n",
      "acknowledged VERB\n",
      ", PUNCT\n",
      "that ADP\n",
      "a DET\n",
      "single ADJ\n",
      "man NOUN\n",
      "in ADP\n",
      "possession NOUN\n",
      "\n",
      " SPACE\n",
      "of ADP\n",
      "a DET\n",
      "good ADJ\n",
      "fortune NOUN\n",
      ", PUNCT\n",
      "must VERB\n",
      "be VERB\n",
      "in ADP\n",
      "want NOUN\n",
      "of ADP\n",
      "a DET\n",
      "wife NOUN\n",
      ". PUNCT\n",
      "\n",
      "\n",
      " SPACE\n",
      "However ADV\n",
      "little ADJ\n",
      "known ADJ\n",
      "the DET\n",
      "feelings NOUN\n",
      "or CCONJ\n",
      "views NOUN\n",
      "of ADP\n",
      "such ADJ\n",
      "a DET\n",
      "man NOUN\n",
      "may VERB\n",
      "be VERB\n",
      "on ADP\n",
      "his ADJ\n",
      "\n",
      " SPACE\n",
      "first ADV\n",
      "entering VERB\n",
      "a DET\n",
      "neighbourhood NOUN\n",
      ", PUNCT\n",
      "this DET\n",
      "truth NOUN\n",
      "is VERB\n",
      "so ADV\n",
      "well ADV\n",
      "fixed VERB\n",
      "in ADP\n",
      "the DET\n",
      "minds NOUN\n",
      "\n",
      " SPACE\n",
      "of ADP\n",
      "the DET\n",
      "surrounding VERB\n",
      "families NOUN\n",
      ", PUNCT\n",
      "that ADP\n",
      "he PRON\n",
      "is VERB\n",
      "considered VERB\n",
      "the DET\n",
      "rightful ADJ\n",
      "property NOUN\n",
      "\n",
      " SPACE\n",
      "of ADP\n",
      "some DET\n",
      "one NUM\n",
      "or CCONJ\n",
      "other ADJ\n",
      "of ADP\n",
      "their ADJ\n",
      "daughters NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in result:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works pretty well!  But for our purposes in this notebook, we're just going to use spaCy to divide updates into sentences, which we can then feed to VADER.  Let's instantiate a VADER analyzer now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the sentiment scores for each sentence:  negative (`neg`), neutral (`neu`), positive (`pos`), and overall sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg': 0.0, 'neu': 0.711, 'pos': 0.289, 'compound': 0.6705},\n",
       " {'neg': 0.0, 'neu': 0.895, 'pos': 0.105, 'compound': 0.6147}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[analyzer.polarity_scores(str(s)) for s in list(result.sents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the first two sentences of _Pride and Prejudice_ score as neutral-to-positive (we don't have a pretrained hilarity detector, alas).  Let's try some raw text from the negative product reviews corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[( This oatmeal is not good.,\n",
       "  {'neg': 0.376, 'neu': 0.624, 'pos': 0.0, 'compound': -0.3412}),\n",
       " (Its mushy, soft, I don't like it.,\n",
       "  {'neg': 0.297, 'neu': 0.703, 'pos': 0.0, 'compound': -0.2755}),\n",
       " (Quaker Oats is the way to go. \n",
       "  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " (Seriously this product was as tasteless as they come.,\n",
       "  {'neg': 0.175, 'neu': 0.825, 'pos': 0.0, 'compound': -0.1779}),\n",
       " (There are much better tasting products out \n",
       "  there but at 100 calories its better than a special k bar or cookie snack pack.,\n",
       "  {'neg': 0.0, 'neu': 0.658, 'pos': 0.342, 'compound': 0.8537}),\n",
       " (You just have to \n",
       "  season it or combine it with something else to share the flavor.\n",
       "  , {'neg': 0.0, 'neu': 0.872, 'pos': 0.128, 'compound': 0.296}),\n",
       " (These were nasty, they were so greasy and too rich for my blood, plus they lacked major flavor, \n",
       "  no spicy jalapeno flavor at all.,\n",
       "  {'neg': 0.191, 'neu': 0.691, 'pos': 0.118, 'compound': -0.296})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = english(\"\"\" This oatmeal is not good. Its mushy, soft, I don't like it. Quaker Oats is the way to go. \n",
    "\n",
    "Seriously this product was as tasteless as they come. There are much better tasting products out \n",
    "there but at 100 calories its better than a special k bar or cookie snack pack. You just have to \n",
    "season it or combine it with something else to share the flavor.\n",
    "\n",
    "These were nasty, they were so greasy and too rich for my blood, plus they lacked major flavor, \n",
    "no spicy jalapeno flavor at all.\n",
    "\"\"\")\n",
    "\n",
    "[(s, analyzer.polarity_scores(str(s))) for s in list(negative.sents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming natural language processing\n",
    "\n",
    "Now we'll connect this sort of analysis to Spark so we can apply it to streaming data.  As before, we're going to load the Kafka connector package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SPARK_VERSION=\"2.2.0\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages org.apache.spark:spark-sql-kafka-0-10_2.11:%s pyspark-shell\" % SPARK_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll set up a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"sentiment test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll deserialize the JSON message payloads into structured data that we can process easily with Spark's data frame operations or structured streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import column, from_json\n",
    "\n",
    "structure = StructType([StructField(fn, StringType(), True) for fn in \"text user_id update_id\".split()])\n",
    "\n",
    "records = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka.kafka.svc:9092\") \\\n",
    "  .option(\"subscribe\", \"social-firehose\") \\\n",
    "  .load() \\\n",
    "  .select(column(\"value\").cast(StringType()).alias(\"value\")) \\\n",
    "  .select(from_json(column(\"value\"), structure).alias(\"json\")) \\\n",
    "  .select(column(\"json.update_id\"), column(\"json.user_id\").alias(\"user_id\"), column(\"json.text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use spaCy and VADER in Spark [user-defined functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf) so that we can \n",
    "\n",
    "1.  write a query that parses longer text in a data frame or structured stream into multiple sentences, and\n",
    "2.  write a query that generates sentiment scores for individual records.\n",
    "\n",
    "Since spaCy depends on a rather large and expensive-to-load model file, we don't want to refer to the model file directly in our user-defined function:  it would be prohibitive to serialize and deserialize it every time we wanted to run the function.  Typically with Spark programs, we'd prefer to [broadcast](https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables) large data like models, but the spaCy model is tricky to serialize.  So instead, we'll use this trick suggested by the [Sparkling Pandas library](https://github.com/sparklingpandas/sparklingml/blob/627c8f23688397a53e2e9e805e92a54c2be1cf3d/sparklingml/transformation_functions.py#L53), essentially simulating lazily-initialized worker-local storage for Spacy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is borrowed from Sparkling Pandas; see here:\n",
    "# https://github.com/sparklingpandas/sparklingml/blob/627c8f23688397a53e2e9e805e92a54c2be1cf3d/sparklingml/transformation_functions.py#L53\n",
    "class SpacyMagic(object):\n",
    "    \"\"\"\n",
    "    Simple Spacy Magic to minimize loading time.\n",
    "    >>> SpacyMagic.get(\"en\")\n",
    "    <spacy.en.English ...\n",
    "    \"\"\"\n",
    "    _spacys = {}\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, lang):\n",
    "        if lang not in cls._spacys:\n",
    "            import spacy\n",
    "            cls._spacys[lang] = spacy.load(lang)\n",
    "        return cls._spacys[lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a user-defined function to split social-media updates into sentences.  We will use spaCy, which is more expensive than most reasonable heuristics for splitting text into sentences (but also much smarter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def split_sentences_impl(s):\n",
    "    \"\"\" splits an English string into sentences, using spaCy \"\"\"\n",
    "    english = SpacyMagic.get(\"en\")\n",
    "    return [str(sentence) for sentence in english(s).sents]\n",
    "\n",
    "split_sentences = udf(split_sentences_impl, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what this looks like, we'll run it on the first 10 rows of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(update_id='00000000000000000000', user_id='4665560161', sentences=[\"Elinor wished that the same forbearance could have extended towards herself, but that was impossible, and she was necessarily drawn from the mother's description.\", '#socialmedia #marketing #yolo']),\n",
       " Row(update_id='00000000000000000000', user_id='1000040647', sentences=['It did not suit her situation or feelings, I might have rejoiced in its termination.', '#tbt #fail #yolo']),\n",
       " Row(update_id='00000000000000000000', user_id='9086078734', sentences=['The furniture was in all probability have gained some news of them; and till we know that she ever should receive another so perfectly gratifying in the occasion and the style.', '#retweet #yolo #ff']),\n",
       " Row(update_id='00000000000000000001', user_id='3082369400', sentences=['After this period every appearance of equal permanency.', '#health']),\n",
       " Row(update_id='00000000000000000001', user_id='5902440326', sentences=['Her performance was pleasing, though by no means tired of wondering and admiring; and not even #Louisa seemed to feel all the tenderness of the past.', '#socialmedia #blogpost']),\n",
       " Row(update_id='00000000000000000001', user_id='3359902759', sentences=['These have NO hydrogenated oils, are fresh and taste delicious.', '#marketing #followfriday #tbt']),\n",
       " Row(update_id='00000000000000000002', user_id='0099016619', sentences=['The post-office has a great desire to see Mr. #Bingley.']),\n",
       " Row(update_id='00000000000000000002', user_id='7761320665', sentences=['Worse than all!', '#health #news']),\n",
       " Row(update_id='00000000000000000002', user_id='8304162681', sentences=['I could not help affronting them.', '#health #retweet']),\n",
       " Row(update_id='00000000000000000003', user_id='2529702535', sentences=['She is netting herself the sweetest cloak you can conceive.', '#ff'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_records = records \\\n",
    "  .orderBy(\"update_id\") \\\n",
    "  .limit(10) \\\n",
    "  .select(\"update_id\", \"user_id\", split_sentences(column(\"text\")).alias(\"sentences\")) \\\n",
    "  .cache()\n",
    "\n",
    "split_records.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explode each array into multiple rows to make further processing easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|update_id           |user_id   |sentence                                                                                                                                                                        |\n",
      "+--------------------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|00000000000000000000|4665560161|Elinor wished that the same forbearance could have extended towards herself, but that was impossible, and she was necessarily drawn from the mother's description.              |\n",
      "|00000000000000000000|4665560161|#socialmedia #marketing #yolo                                                                                                                                                   |\n",
      "|00000000000000000000|1000040647|It did not suit her situation or feelings, I might have rejoiced in its termination.                                                                                            |\n",
      "|00000000000000000000|1000040647|#tbt #fail #yolo                                                                                                                                                                |\n",
      "|00000000000000000000|9086078734|The furniture was in all probability have gained some news of them; and till we know that she ever should receive another so perfectly gratifying in the occasion and the style.|\n",
      "|00000000000000000000|9086078734|#retweet #yolo #ff                                                                                                                                                              |\n",
      "|00000000000000000001|3082369400|After this period every appearance of equal permanency.                                                                                                                         |\n",
      "|00000000000000000001|3082369400|#health                                                                                                                                                                         |\n",
      "|00000000000000000001|5902440326|Her performance was pleasing, though by no means tired of wondering and admiring; and not even #Louisa seemed to feel all the tenderness of the past.                           |\n",
      "|00000000000000000001|5902440326|#socialmedia #blogpost                                                                                                                                                          |\n",
      "|00000000000000000001|3359902759|These have NO hydrogenated oils, are fresh and taste delicious.                                                                                                                 |\n",
      "|00000000000000000001|3359902759|#marketing #followfriday #tbt                                                                                                                                                   |\n",
      "|00000000000000000002|0099016619|The post-office has a great desire to see Mr. #Bingley.                                                                                                                         |\n",
      "|00000000000000000002|7761320665|Worse than all!                                                                                                                                                                 |\n",
      "|00000000000000000002|7761320665|#health #news                                                                                                                                                                   |\n",
      "|00000000000000000002|8304162681|I could not help affronting them.                                                                                                                                               |\n",
      "|00000000000000000002|8304162681|#health #retweet                                                                                                                                                                |\n",
      "|00000000000000000003|2529702535|She is netting herself the sweetest cloak you can conceive.                                                                                                                     |\n",
      "|00000000000000000003|2529702535|#ff                                                                                                                                                                             |\n",
      "+--------------------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "sentences = split_records.select(\"update_id\", \"user_id\", explode(column(\"sentences\")).alias(\"sentence\"))\n",
    "sentences.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our user-defined function for VADER scoring:  it will take text and return a sentiment structure.  Note that we _are_ actually creating a broadcast variable for the VADER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "sentiment_fields = \"pos neg neu compound\".split()\n",
    "sentiment_structure = StructType([StructField(fn, FloatType(), True) for fn in sentiment_fields])\n",
    "\n",
    "analyzer_bcast = spark.sparkContext.broadcast(analyzer)\n",
    "\n",
    "def vader_impl(s):\n",
    "    va = analyzer_bcast.value\n",
    "    result = va.polarity_scores(s)\n",
    "    return [result[key] for key in sentiment_fields]\n",
    "\n",
    "sentiment_score = udf(vader_impl, sentiment_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can annotate each sentence with its sentiment and order from most negative to most positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+\n",
      "|           update_id|   user_id|            sentence|           sentiment|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "|00000000000000000002|7761320665|     Worse than all!|[0.0,0.629,0.371,...|\n",
      "|00000000000000000002|8304162681|I could not help ...|[0.0,0.361,0.639,...|\n",
      "|00000000000000000000|4665560161|Elinor wished tha...|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000000|1000040647|    #tbt #fail #yolo|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000002|8304162681|    #health #retweet|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000000|9086078734|  #retweet #yolo #ff|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000000|4665560161|#socialmedia #mar...|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000001|3359902759|#marketing #follo...|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000002|7761320665|       #health #news|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000003|2529702535|She is netting he...|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000003|2529702535|                 #ff|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000001|3082369400|After this period...|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000001|3082369400|             #health|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000001|5902440326|#socialmedia #blo...|   [0.0,0.0,1.0,0.0]|\n",
      "|00000000000000000000|1000040647|It did not suit h...|[0.188,0.0,0.812,...|\n",
      "|00000000000000000001|3359902759|These have NO hyd...|[0.377,0.184,0.43...|\n",
      "|00000000000000000001|5902440326|Her performance w...|[0.252,0.146,0.60...|\n",
      "|00000000000000000002|0099016619|The post-office h...|[0.493,0.0,0.507,...|\n",
      "|00000000000000000000|9086078734|The furniture was...|[0.292,0.0,0.708,...|\n",
      "+--------------------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences \\\n",
    "  .select(\"update_id\", \"user_id\", \"sentence\", sentiment_score(column(\"sentence\")).alias(\"sentiment\")) \\\n",
    "  .orderBy(\"sentiment.compound\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
